
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="../../maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
<style type="text/css"> 
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  .link_preview {
		border: 0px solid #ddd; /* Gray border */
		border-radius: 4px;  /* Rounded border */
		padding: 5px; /* Some padding */
	 /* Set a small width */
	}

  .link_preview:hover {
		box-shadow: 0 0 2px 1px rgba(0, 140, 186, 0.5);
	}

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->
    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114291442-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-3');
</script>



<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>


<!-- Mirrored from devendrachaplot.github.io/projects/semantic-exploration by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 12 Sep 2024 15:47:13 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="resources/favicon.ico">
  <title>Finding Everything: A General Approach to Multi-object Search with Vision Language Models</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://www.youtube.com/iframe_api"></script>
  
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;width:600px">Find Everything: A General Vision Language Model Approach to Multi-Object Search</span></center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="http://jeongwoongc.github.io" target="_blank">Daniel Choi</a><sup>1</sup></span></center></td>
        
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://angusfung.github.io" target="_blank">Angus Fung</a><sup>1,2</sup></span></center></td>
          
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://scholar.google.ca/citations?user=LA6TYrgAAAAJ&hl=en&authuser=1" target="_blank">Haitong Wang</a><sup>1</sup></span></center></td>
        
        <td align=center width=200px>
        <center><span style="font-size:22px"><a href="https://aarontan-git.github.io" target="_blank">Aaron Hao Tan</a><sup>1,2</sup></span></center></td>
      </tr>
      </table>
    
      <table align=center width=800px>
        <tr>
          <td align=center width=800px>
            <center>
              <span style="font-size:20px"><sup>1</sup>University of Toronto, <sup>2</sup><a href="https://www.syncereai.com" target="_blank">SyncereAI</a></span>
            </center>
          </td>
        </tr>
      </table>
    
      <table align=center width=800px>
        <tr>
          <td align=center width=800px><center><span style="font-size:22px">Equal contribution</span></center></td>
        </tr>
      </table><br/>
    
      <table align=center width=700px>
          <tr>
            <td align=center width=100px><center><span style="font-size:28px"><a href="#">[Paper]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="#">[Code] (Coming Soon!)</a></span></center></td>
          <tr/>
      </table><br/>

<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=300px>
      <tr><td align=center width=300px>
<!--      <iframe width="768" height="432" src="https://youtu.be/tlyz68j_jvE" frameborder="0" allowfullscreen></iframe>-->
<!--          <iframe width="768" height="432" src="https://www.youtube.com/embed/tlyz68j_jvE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
          <center><img src = "resources/finder.png" height="400px"></img></a><br></center>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify"> 
          Efficient navigation and search in unknown environments for multiple objects is a fundamental challenge in robotics, particularly in applications such as warehouse automation, domestic assistance, and search-and-rescue. The Multi-Object Search (MOS) problem involves finding an optimal sequence of locations to maximize the probability of discovering target objects while minimizing travel costs. In this paper, we introduce a novel approach to the MOS problem, called Finder, which leverages vision language models (VLM) to locate multiple objects across diverse environments. Specifically, our approach combines semantic mapping with spatio-probabilistic reasoning and adaptive planning. We enhance object recognition and scene understanding through the integration of VLM, constructing an episodic semantic map to guide efficient exploration based on multiple object goals. Our extensive experiments in both simulated and real-world environments demonstrate Finder's superior performance compared to existing multi-object search methods using deep reinforcement learning and VLM. Additional ablation and scalability studies highlight the importance of our design choices in addressing the inherent uncertainty and complexity of the MOS problem.
      </div>
      <br><hr>
      <center><h1>Multi-Object Search</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        The proposed model, <i>Finder</i>, consists of four core modules: Object Detector, Spatial Map Generator, Score Map Generator, and Exploration Planner. The Object Detector identifies scene and target objects using YOLOv7 and Grounding-DINO, generating segmentation masks and determining the closest points in 3D space. The Spatial Map Generator creates occupancy and semantic maps from RGB and depth images to represent the environment. The Score Map Generator computes scene-to-object and object-to-object correlation scores, fusing them into a unified score map that guides exploration. The Exploration Planner selects frontiers based on utility scores, combining correlation scores and distance to unexplored areas, and navigates the robot using A* and Timed Elastic Band planners. This approach efficiently tracks and reasons about multiple objects in complex environments.
      </div><br/>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "resources/finder_arch.png" width="800px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr>


      <!--<center><h1>Demo Video</h1></center>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="640" height="360" src="https://www.youtube.com/embed/h56dA2uxpGU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br><hr>

      <center><h1>Real-World Transfer</h1></center>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><img src = "resources/semexp_real_world.gif" width="640px"></img><br></center>
        </td></tr>
      </table>
      <br/><hr>-->

	<!--<center><h1>Media</h1></center>
	<div style="width:800px; margin:0 auto; text-align:justify">
        A short video created by CMU Media team describing the key ideas behind this project.   
     	 </div>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/FhIut4bqFyw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>

      <br/>-->
 	
	<!--<div style="width:800px; margin:0 auto; text-align:justify">
        Read more about the project in the following media articles:   
     	 </div>
	<table align=center width=700px>
	          <tr>
            <td align=left width=120px><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://techcrunch.com/2020/07/20/cmu-and-facebook-ai-research-use-machine-learning-to-teach-robots-to-navigate-by-recognizing-objects/" onkeypress="window.open(this.href); return false;">
									<img src="logos/tc.png" class="link_preview" 	style="max-width:130px;">
									</a></span></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://gizmodo.com/teaching-robots-there-are-no-toilets-in-the-kitchen-mak-1844446396" onkeypress="window.open(this.href); return false;">
									<img src="logos/gizmodo.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
            <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.thehindu.com/sci-tech/technology/a-navigation-system-powered-by-machine-learning-is-training-robots-to-recognise-objects/article32218732.ece" onkeypress="window.open(this.href); return false;">
									<img src="logos/hindu.png" class="link_preview" style="max-width:150px;">
									</a></span></center></td>
	    <td align=center width=120px><center><span style="font-size:28px"><a onclick="window.open(this.href); return false;" href="https://www.cmu.edu/news/stories/archives/2020/july/robot-navigation.html" onkeypress="window.open(this.href); return false;">
									<img src="logos/cmu.png" class="link_preview" style="max-width:150px;">
						</a></span></center></td>
          <tr/>
      </table>
      <br/><hr>-->
      <!--<center id="sourceCode"><h1>Source Code and Pre-trained models</h1></center>
      <div style="width:800px; margin:0 auto; text-align:center">
      We have released the PyTorch implementation of Goal-Oriented Semantic Exploration system along with pre-trained models on GitHub. Try our code!
      </div>
      <table align=center width=900px>
        <tr>
          <td width=300px align=center>
            <span style="font-size:28px"><a href='https://github.com/devendrachaplot/Object-Goal-Navigation/'>[GitHub]</a></span>
          </td>
        </tr>
      </table>
      <br><hr>-->




      <!--<table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=200px align=left>
        <a href="papers/cvpr20_neural_topological_slam.html"><img style="width:200px" src="resources/thumbnail_semexp.jpg"/></a>
        <center>
        <span style="font-size:20pt"><a href="https://arxiv.org/pdf/2007.00643.pdf">[Paper]</a>
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
      <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Chaplot, D.S., Gandhi, D., Gupta, A. and  Salakhutdinov, R. 2020. <br>
            Object Goal Navigation using Goal-Oriented Semantic Exploration.
            <br> <em>In Neural Information Processing Systems (NeurIPS-20).</em> </span></p>
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span>
        <div class="paper" id="assemblies19_bib">
                <pre xml:space="preserve">
@inproceedings{chaplot2020object,
  title={Object Goal Navigation using Goal-Oriented Semantic Exploration},
  author={Chaplot, Devendra Singh and Gandhi, Dhiraj and
            Gupta, Abhinav and Salakhutdinov, Ruslan},
  booktitle={In Neural Information Processing Systems},
  year={2020}}
                </pre>
          </div>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          
          </td>
          </tr>
      </table>
    <br><hr>-->


    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      This work was supported by <a href="https://www.syncereai.com">SyncereAI</a>.
    <br>
          Website template from <a href="https://devendrachaplot.github.io/projects/semantic-exploration">here</a>. <br>
      </left></td></tr>
    </table>
  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>

</html>

